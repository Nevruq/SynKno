{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf08607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import chromadb_handler as CH\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c048fafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# turn true if rerun of data\n",
    "cast_data = False\n",
    "\n",
    "if cast_data:\n",
    "    directory_str = \"data/DUUIDataset/training\"\n",
    "    directory = os.fsencode(directory_str)\n",
    "    counter = 1    \n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        print(filename)\n",
    "        if filename.endswith(\".pdf\") : \n",
    "            PDF_PATH = directory_str + \"/\" + filename\n",
    "            CHUNK_SIZE = 300  # Anzahl Zeichen, ≈ 500–700 Tokens\n",
    "\n",
    "            # -----------------------------\n",
    "            # 1. PDF Laden\n",
    "            # -----------------------------\n",
    "            pdf_reader = PyPDF2.PdfReader(PDF_PATH)\n",
    "\n",
    "            # Metadaten aus PDF\n",
    "            meta = pdf_reader.metadata\n",
    "\n",
    "            title = meta.title if meta and meta.title else \"Unknown Title\"\n",
    "            authors = meta.author.split(\",\") if meta and meta.author else []\n",
    "\n",
    "            # -----------------------------\n",
    "            # 2. Seiteninhalt extrahieren\n",
    "            # -----------------------------\n",
    "            pages = []\n",
    "            for i, page in enumerate(pdf_reader.pages):\n",
    "                text = page.extract_text()\n",
    "                pages.append({\"page\": i+1, \"text\": text})\n",
    "\n",
    "            # -----------------------------\n",
    "            # 3. Chunks bauen\n",
    "            # -----------------------------\n",
    "            def chunk_text(text, size=CHUNK_SIZE):\n",
    "                chunks = []\n",
    "                for i in range(0, len(text), size):\n",
    "                    chunks.append(text[i:i+size])\n",
    "                return chunks\n",
    "\n",
    "            dataset = []\n",
    "            doc_id = Path(PDF_PATH).stem\n",
    "\n",
    "            for page in pages:\n",
    "                text = page[\"text\"]\n",
    "                if not text:\n",
    "                    continue\n",
    "\n",
    "                chs = chunk_text(text)\n",
    "\n",
    "                for idx, chunk in enumerate(chs):\n",
    "                    entry = {\n",
    "                        \"id\": doc_id,\n",
    "                        \"chunk_id\": f\"{doc_id}_p{page['page']}_c{idx}\",\n",
    "                        \"source\": PDF_PATH,\n",
    "                        \"title\": title,\n",
    "                        \"authors\": authors,\n",
    "                        \"publication_year\": \"None\",\n",
    "                        \"page_start\": page[\"page\"],\n",
    "                        \"page_end\": page[\"page\"],\n",
    "                        \"text\": chunk,\n",
    "                        \"embedding\": None\n",
    "                    }\n",
    "                    dataset.append(entry)\n",
    "\n",
    "            # -----------------------------\n",
    "            # 4. JSON exportieren\n",
    "            # -----------------------------\n",
    "            with open(f\"rag_dataset_{counter}.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "                for item in dataset:\n",
    "                    f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "            counter += 1\n",
    "            print(\"FERTIG! Datei rag_dataset.jsonl generiert.\")\n",
    "\n",
    "\n",
    "\n",
    "            continue\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "405b9ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "ch = CH.chromaDBWrapper()\n",
    "client = chromadb.PersistentClient(\"chroma\")\n",
    "client.get_or_create_collection(name=\"DUUI_300\")\n",
    "collection = client.get_collection(\"DUUI_300\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abd50256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert JSONL data into the ChromaDB\n",
    "\n",
    "import PyPDF2\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# turn true if rerun of data\n",
    "cast_data_DUUI = False\n",
    "\n",
    "if cast_data_DUUI:\n",
    "    embedding_structure = []\n",
    "    ids_fl = []  \n",
    "    metadatas_fl = []\n",
    "    documents_fl = []\n",
    "    uris_fl = []\n",
    "    if cast_data:\n",
    "        directory_str = \"data/DUUIDataset/training\"\n",
    "        directory = os.fsencode(directory_str)\n",
    "        counter = 1  \n",
    "        \n",
    "        for file in os.listdir(directory):\n",
    "            filename = os.fsdecode(file)\n",
    "            str_filename = directory_str + \"/\" + filename\n",
    "            print(str_filename)\n",
    "            if filename.endswith(\".jsonl\") : \n",
    "                with open(str_filename) as f:\n",
    "                    data = [json.loads(line) for line in f]\n",
    "                    for line in data:\n",
    "                        #ids\n",
    "                        ids_fl.append(line[\"chunk_id\"])\n",
    "                        # metadeta\n",
    "                        if isinstance(authors, list):\n",
    "                            authors = \", \".join(authors)\n",
    "                        metadeta_dict = {\"title\": line[\"title\"], \n",
    "                                        \"authors\": authors, \n",
    "                                        \"publication_year\": line[\"publication_year\"], \n",
    "                                        \"page_start\": line[\"page_start\"], \n",
    "                                        \"page_end\": line[\"page_end\"]}\n",
    "                        metadatas_fl.append(metadeta_dict)\n",
    "                        # Docuemnts / text\n",
    "                        documents_fl.append(line[\"text\"])\n",
    "                        # Uris\n",
    "                        uris_fl.append(line[\"source\"])\n",
    "                #cast JSON arugment into proper Form\n",
    "\n",
    "\n",
    "                continue\n",
    "            else:\n",
    "                continue\n",
    "    #add metadatas\n",
    "    collection.add(ids=ids_fl, metadatas=metadatas_fl, documents=documents_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be8cd296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf = pd.read_csv(\"data/BBCNews/bbc_news.csv\")\\ndf[\"parsed_date\"] = df[\"pubDate\"].apply(\\n    lambda x: datetime.strptime(x, \"%a, %d %b %Y %H:%M:%S %Z\")\\n)\\n# Extract the year\\ndf[\"year\"] = df[\"parsed_date\"].dt.year\\n\\n# Filter dataset for everything >= 2023\\nfiltered_df = df[df[\"year\"] >= 2023]\\nsmapled_df = filtered_df.sample(n=5000, random_state=42)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df = pd.read_csv(\"data/BBCNews/bbc_news.csv\")\n",
    "df[\"parsed_date\"] = df[\"pubDate\"].apply(\n",
    "    lambda x: datetime.strptime(x, \"%a, %d %b %Y %H:%M:%S %Z\")\n",
    ")\n",
    "# Extract the year\n",
    "df[\"year\"] = df[\"parsed_date\"].dt.year\n",
    "\n",
    "# Filter dataset for everything >= 2023\n",
    "filtered_df = df[df[\"year\"] >= 2023]\n",
    "smapled_df = filtered_df.sample(n=5000, random_state=42)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8536651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "\n",
    "cast_data_bbc = False\n",
    "if cast_data_bbc:\n",
    "    ids_b = []\n",
    "    metadatas_b = []\n",
    "    docuemnts_b = []\n",
    "    unique_set = set()\n",
    "    for row in smapled_df.itertuples(index=False):\n",
    "        # Create a short hash from full URL\n",
    "        # form year_shortURl_Hash of whole URL\n",
    "        # create unique ID\n",
    "        domain = urlparse(row.link).netloc.replace(\".\", \"_\")\n",
    "        hash_part = hashlib.md5((row.link+row.title).encode(\"utf-8\")).hexdigest()[:16]\n",
    "        date = row.pubDate\n",
    "        id = hash_part+\"_\"+str(row.year)\n",
    "        # text/documents\n",
    "        document = row.description\n",
    "        #metadata\n",
    "        metadeta_dict = {\"title\": row.title, \n",
    "                        \"authors\": authors, \n",
    "                                        \"publication_date\": row.pubDate, \n",
    "                                        \"link\" : row.link,\n",
    "                                        \"guide\": row.guid}\n",
    "\n",
    "        # check if id (hash of the )\n",
    "        if id not in unique_set:\n",
    "            unique_set.add(id)\n",
    "            # if not in unique set\n",
    "            metadatas_b.append(metadeta_dict)\n",
    "            docuemnts_b.append(row.description)\n",
    "            ids_b.append(id)\n",
    "\n",
    "    client.delete_collection(name=\"bbc_news\")\n",
    "    collection_b = client.get_or_create_collection(name=\"bbc_news\")\n",
    "    collection_b.add(ids=ids_b, metadatas=metadatas_b, documents=docuemnts_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04639c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:    \n",
    "# Filter data for Stocknews\n",
    "    df = pd.read_csv(\"data/stockNews/nasdaq_news.csv\", nrows=20000)\n",
    "    df[\"parsed_date\"] = df[\"Date\"].apply(\n",
    "        lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S %Z\")\n",
    "    )\n",
    "    #2023-12-11 00:00:00 UTC\n",
    "    #Sun, 06 Mar 2022 22:49:58 GMT\n",
    "    # Extract the year\n",
    "    df[\"year\"] = df[\"parsed_date\"].dt.year\n",
    "\n",
    "    # Filter dataset for everything >= 2023\n",
    "    filtered_df = df[df[\"year\"] >= 2023]\n",
    "    smapled_df = filtered_df.sample(n=5000, random_state=42)\n",
    "    len(smapled_df)\n",
    "    smapled_df.to_csv(\"filtered_nasdaq_news.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bec9ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data for Nasdaq News\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "smapled_df = pd.read_csv(\"filtered_nasdaq_news.csv\")\n",
    "\n",
    "def chunk_text_tiktoken(text, max_tokens=400):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(tokens), max_tokens):\n",
    "        chunk = tokens[i:i + max_tokens]\n",
    "        chunks.append(tokenizer.decode(chunk))\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "batch_size = 5400\n",
    "token_length = 400\n",
    "ids_sto = []\n",
    "metadatas_sto = []\n",
    "documents_sto = []\n",
    "unique_set_sto = set()\n",
    "cast_nasdaq_data = True\n",
    "if cast_nasdaq_data:\n",
    "    \n",
    "    for row in smapled_df.itertuples(index=False):\n",
    "        if len(ids_sto) > 5500:\n",
    "            continue\n",
    "        # Create a short hash from full URL\n",
    "        # form year_shortURl_Hash of whole URL\n",
    "        # create unique ID\n",
    "        chunks_row = chunk_text_tiktoken(row.Article)\n",
    "        for i, chunk in enumerate(chunks_row, 1):\n",
    "            hash_part = hashlib.md5((row.Article).encode(\"utf-8\")).hexdigest()[:16]\n",
    "            date = row.Date\n",
    "            id = hash_part+\"_\"+str(row.year)+\"_\"+str(i)\n",
    "            # text/documents\n",
    "            document = chunk\n",
    "            #metadata\n",
    "            metadeta_dict = {\"title\": row.Article_title, \n",
    "                                \"stock\": row.Stock_symbol,\n",
    "                                \"publication_date\": row.year,\n",
    "                                \"part_of_article\": i\n",
    "                            }\n",
    "            # check if id (hash of the )\n",
    "\n",
    "            if id not in unique_set_sto:\n",
    "                unique_set_sto.add(id)\n",
    "                # if not in unique set\n",
    "                metadatas_sto.append(metadeta_dict)\n",
    "                documents_sto.append(document)\n",
    "                ids_sto.append(id)\n",
    "        \n",
    "    collection_b = client.get_or_create_collection(name=\"nasdaq_news_chunked_new\")\n",
    "    collection_b.add(ids=ids_sto[:batch_size], metadatas=metadatas_sto[:batch_size], documents=documents_sto[:batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6b129a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': \"Based on the provided information, what were CSX's cash and cash equivalents at the end of 2022, how did this figure relate to its current debt, and what conclusion does the text draw from this comparison?\", 'answer': \"CSX's cash and cash equivalents were $2,087 million at the end of 2022, while its current debt was $151 million, implying that the company has sufficient cash to meet its current debt obligations.\"}\n",
      "row added1\n",
      "{'question': \"According to the report, what EU rule does Apple's anti-steering obligation allegedly breach, and which company filed the 2019 complaint that sparked the case?\", 'answer': 'EU rules against unfair trading conditions; Spotify.'}\n",
      "row added2\n",
      "{'question': 'What two changes does the SAVE repayment plan introduce for undergraduate loans, as described by Matt Frankel, regarding discretionary income payments and the poverty-line threshold?', 'answer': 'Discretionary payments are reduced from 10% to 5% of discretionary income, and the threshold for discretionary income is raised from 150% to 225% of the federal poverty line.'}\n",
      "row added3\n",
      "{'question': 'What position will Natascha Viljoen assume after stepping down as CEO of Anglo American Platinum, and at which company will she take this role?', 'answer': 'Chief operating officer at Newmont Corporation.'}\n",
      "row added4\n",
      "{'question': 'According to the report, what factors contributed to the S&P 500 and Nasdaq closing weaker on Tuesday?', 'answer': \"It was the first day of seasonally slow August, ahead of U.S. jobs data and major companies' earnings reports later this week.\"}\n",
      "row added5\n",
      "{'question': \"What was Apple's revenue in the second quarter, and what notable achievement did Apple report for its services revenue despite the challenging macroeconomic environment?\", 'answer': 'Revenue was $94.8 billion in the second quarter, and services revenue posted a record.'}\n",
      "row added6\n",
      "{'question': 'According to the article, which five ETFs dominated the top creation list last week?', 'answer': 'Invesco QQQ Trust (QQQ); iShares Core U.S. Aggregate Bond ETF (AGG); Health Care Select Sector SPDR Fund (XLV); Vanguard S&P 500 ETF (VOO); BondBloxx Bloomberg One Year Target Duration US Treasury ETF (XONE).'}\n",
      "row added7\n",
      "{'question': 'Among the three dividend stocks highlighted—Microsoft, Apple, and Starbucks—which company is noted to have a 12-year streak of annual dividend increases and currently yields about 2%?', 'answer': 'Starbucks.'}\n",
      "row added8\n",
      "{'question': \"According to ETF Channel's analysis of PBUS's underlying holdings, what is the implied 12-month target price per unit for the Invesco PureBeta MSCI USA ETF (PBUS), and what is the corresponding upside relative to its recent price?\", 'answer': 'Implied target price: $46.02 per unit; Upside: 12.23%.'}\n",
      "row added9\n",
      "{'question': \"Which real estate investment trust is described as 'The Monthly Dividend Company' and has a 4.9% dividend yield in the article?\", 'answer': 'Realty Income (NYSE:O)'}\n",
      "row added10\n",
      "{'question': 'Which ETF recorded the largest week-over-week inflow, and what was the size of that inflow in units and its percentage change?', 'answer': 'The SPDR Portfolio S&P 500 ETF (SPLG) recorded the largest week-over-week inflow, adding 14,900,000 units, a 3.7% increase.'}\n",
      "row added11\n",
      "CSV-Erstellung abgeschlossen.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import importlib\n",
    "import call_llm\n",
    "importlib.reload(call_llm) # Beibehalten, falls erforderlich\n",
    "import pandas as pd # Hinzugefügt, da Sie es im Originalcode referenzieren\n",
    "import os\n",
    "import csv # NEU: Importiere das CSV-Modul\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# loading variables from .env file\n",
    "load_dotenv() # pass a path if it's not a .env in the current working directory \n",
    "\n",
    "# Setup:\n",
    "LLM_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "df = pd.read_csv(\"filtered_nasdaq_news.csv\") # DataFrame wird geladen\n",
    " # Angenommen, das ist der DataFrame, den Sie verwenden\n",
    "\n",
    "counter = 0\n",
    "test = 0\n",
    "context = \"\"\n",
    "\n",
    "# Öffnen der Datei mit dem 'csv'-Modul\n",
    "with open(\"QA_nasdaq.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as output_file:\n",
    "    # NEU: Initialisiere den CSV-Writer\n",
    "    csv_writer = csv.writer(output_file, delimiter=',')\n",
    "    \n",
    "    # Schreibe die Kopfzeile\n",
    "    csv_writer.writerow([\"question\", \"answer\"]) \n",
    "    \n",
    "    for row in df.itertuples(index=False):\n",
    "        if test > 10:\n",
    "             break # Verwende 'break' anstelle von 'continue', um die Schleife zu beenden\n",
    "        \n",
    "        context += row.Article\n",
    "        counter += 1\n",
    "        \n",
    "        if counter == 5:\n",
    "              # Annahme: call_llm.gen_QA_nasdaq gibt den JSON-String korrekt zurück\n",
    "              llm_answer = call_llm.gen_QA_nasdaq(model=LLM_client, context=context).output_text\n",
    "              context = \"\"\n",
    "              counter = 0\n",
    "              test += 1\n",
    "              \n",
    "              try:\n",
    "                  llm_answer_json = json.loads(llm_answer)\n",
    "                  question = llm_answer_json.get(\"question\", \"\")\n",
    "                  answer = llm_answer_json.get(\"answer\", \"\")\n",
    "                  \n",
    "                  print(llm_answer_json)\n",
    "                  \n",
    "                  # NEU: Schreibe die Zeile mit dem CSV-Writer\n",
    "                  # Das CSV-Modul kümmert sich um die korrekte Formatierung (Anführungszeichen, Trennzeichen)\n",
    "                  csv_writer.writerow([question, answer])\n",
    "                  print(\"row added\"+ str(test))\n",
    "              except json.JSONDecodeError as e:\n",
    "                  print(f\"Fehler beim Dekodieren von JSON: {e}. Antwort: {llm_answer}\")\n",
    "              except Exception as e:\n",
    "                  print(f\"Ein unerwarteter Fehler ist aufgetreten: {e}\")\n",
    "\n",
    "# Die Datei wird automatisch geschlossen, da 'with open' verwendet wird\n",
    "print(\"CSV-Erstellung abgeschlossen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e35e3f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Based on the provided information, what were C...</td>\n",
       "      <td>CSX's cash and cash equivalents were $2,087 mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>According to the report, what EU rule does App...</td>\n",
       "      <td>EU rules against unfair trading conditions; Sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What two changes does the SAVE repayment plan ...</td>\n",
       "      <td>Discretionary payments are reduced from 10% to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What position will Natascha Viljoen assume aft...</td>\n",
       "      <td>Chief operating officer at Newmont Corporation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>According to the report, what factors contribu...</td>\n",
       "      <td>It was the first day of seasonally slow August...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What was Apple's revenue in the second quarter...</td>\n",
       "      <td>Revenue was $94.8 billion in the second quarte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>According to the article, which five ETFs domi...</td>\n",
       "      <td>Invesco QQQ Trust (QQQ); iShares Core U.S. Agg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Among the three dividend stocks highlighted—Mi...</td>\n",
       "      <td>Starbucks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>According to ETF Channel's analysis of PBUS's ...</td>\n",
       "      <td>Implied target price: $46.02 per unit; Upside:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Which real estate investment trust is describe...</td>\n",
       "      <td>Realty Income (NYSE:O)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Which ETF recorded the largest week-over-week ...</td>\n",
       "      <td>The SPDR Portfolio S&amp;P 500 ETF (SPLG) recorded...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   Based on the provided information, what were C...   \n",
       "1   According to the report, what EU rule does App...   \n",
       "2   What two changes does the SAVE repayment plan ...   \n",
       "3   What position will Natascha Viljoen assume aft...   \n",
       "4   According to the report, what factors contribu...   \n",
       "5   What was Apple's revenue in the second quarter...   \n",
       "6   According to the article, which five ETFs domi...   \n",
       "7   Among the three dividend stocks highlighted—Mi...   \n",
       "8   According to ETF Channel's analysis of PBUS's ...   \n",
       "9   Which real estate investment trust is describe...   \n",
       "10  Which ETF recorded the largest week-over-week ...   \n",
       "\n",
       "                                               answer  \n",
       "0   CSX's cash and cash equivalents were $2,087 mi...  \n",
       "1   EU rules against unfair trading conditions; Sp...  \n",
       "2   Discretionary payments are reduced from 10% to...  \n",
       "3     Chief operating officer at Newmont Corporation.  \n",
       "4   It was the first day of seasonally slow August...  \n",
       "5   Revenue was $94.8 billion in the second quarte...  \n",
       "6   Invesco QQQ Trust (QQQ); iShares Core U.S. Agg...  \n",
       "7                                          Starbucks.  \n",
       "8   Implied target price: $46.02 per unit; Upside:...  \n",
       "9                              Realty Income (NYSE:O)  \n",
       "10  The SPDR Portfolio S&P 500 ETF (SPLG) recorded...  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"QA_nasdaq.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a9cd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'pipeline' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[135]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m results_list = []\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# init chromadbHandler\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m ch_handler = \u001b[43mch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mChromaDBHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m QA_nasdaq.itertuples(index=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     42\u001b[39m    question = row.question\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Bachelor/SynKno/src/chromadb_handler.py:11\u001b[39m, in \u001b[36mChromaDBHandler.__init__\u001b[39m\u001b[34m(self, collection)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, collection):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     pipeline = \u001b[43mpipeline\u001b[49m(\n\u001b[32m     12\u001b[39m         task=\u001b[33m\"\u001b[39m\u001b[33mfill-mask\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m         model=\u001b[33m\"\u001b[39m\u001b[33mgoogle-bert/bert-base-uncased\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m         dtype=torch.float16,\n\u001b[32m     15\u001b[39m         device=\u001b[32m0\u001b[39m\n\u001b[32m     16\u001b[39m         )\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mself\u001b[39m.tokenized_docs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mself\u001b[39m.collection = collection\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'pipeline' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "#import wikipedia\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer\n",
    "import torch\n",
    "import cBert.wrapper_CBert as cBert\n",
    "import weighting_prompt as wp\n",
    "import chromadb_handler as ch\n",
    "import call_llm\n",
    "import json\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import rag_data\n",
    "importlib.reload(call_llm)\n",
    "importlib.reload(rag_data)\n",
    "importlib.reload(ch)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   # init model\n",
    "   MODEL_NAME_2 = \"gpt-5-nano-2025-08-07\"\n",
    "   MODEL_NAME = \"gpt-3.5-turbo-0125\"\n",
    "   load_dotenv()\n",
    "   LLM_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "   # 1. Stelle eine Frage \n",
    "   \n",
    "   # Open output file\n",
    "   # write header: Question, Ideal_answer, reponse_1{metadaten}, response_2{metadaten}, comparission{metadaten}\n",
    "\n",
    "   client = chromadb.PersistentClient(\"src/chroma\")\n",
    "   collection = client.get_or_create_collection(\"nasdaq_news_chunked\")\n",
    "\n",
    "   data_file = open(\"data_compare.csv\", \"w\", encoding=\"utf-8\")\n",
    "   QA_nasdaq = pd.read_csv(\"QA_nasdaq.csv\", sep=\",\")\n",
    "   results_list = []\n",
    "\n",
    "   # init chromadbHandler\n",
    "\n",
    "\n",
    "   for row in QA_nasdaq.itertuples(index=False):\n",
    "      question = row.question\n",
    "      ideal_answer = row.answer\n",
    "      \n",
    "      # Method 1: topics_formatted, query_summary_results, answer_llm_chunked_rag\n",
    "      response_categories = call_llm.call_llm_categories(row.question, LLM_client)\n",
    "      response_categories_json = json.loads(response_categories.output_text)\n",
    "\n",
    "      topics_formatted = [response_categories_json[\"subQuerie1\"], response_categories_json[\"subQuerie2\"], response_categories_json[\"subQuerie3\"]]\n",
    "      query_summary_results = collection.query(query_texts=topics_formatted, n_results=3)\n",
    "      query_summary_results_formatted = query_summary_results[\"documents\"][0]\n",
    "      answer_llm_Chunked_Rag = call_llm.call_llm_answer_RAG(query_results=query_summary_results_formatted, user_prompt=question, model=LLM_client)\n",
    "\n",
    "      # Method 2: Question, query_regular_rag,  answer_llm_wRag\n",
    "      query_regular_rag = collection.query(query_texts=question, n_results=3)\n",
    "      context_query = query_regular_rag[\"documents\"][0]\n",
    "      asnwer_llm_wRAG = call_llm.call_llm_answer_RAG(query_results=context_query, user_prompt=question, model=LLM_client)\n",
    "\n",
    "      # Comparisson\n",
    "      response_compare = call_llm.call_llm_compare_answers(prompt=question,\n",
    "                                                         ideal_answer=ideal_answer,\n",
    "                                                         answer_destilled=asnwer_llm_wRAG.output_text, \n",
    "                                                         answer_regular=answer_llm_Chunked_Rag.output_text,\n",
    "                                                         model=LLM_client)\n",
    "      # create json with data class\n",
    "      ragData = rag_data.ragData(question=question, ideal_answer=ideal_answer, \n",
    "                           comparisson=response_compare.output_text)\n",
    "      formatted_response_compare = json.loads(response_compare.output_text)\n",
    "\n",
    "      row_dict = {\"question\": question,\n",
    "                  \"answer\": answer,\n",
    "                  \"score_1\": formatted_response_compare[\"answer1_score\"],\n",
    "                  \"score_2\": formatted_response_compare[\"answer2_score\"]\n",
    "                  }\n",
    "      results_list.append(row_dict)\n",
    "      \n",
    "      break\n",
    "\n",
    "df_results = pd.DataFrame(results_list)\n",
    "df_results.to_csv(\n",
    "        \"output_v2.csv\", \n",
    "        sep=';', \n",
    "        index=False, \n",
    "        encoding='utf-8'\n",
    "        )\n",
    "output_file.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2115ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3181818181818182\n",
      "0.13636363636363635\n"
     ]
    }
   ],
   "source": [
    "data_rag = pd.read_csv(\"output_v2.csv\", sep=\";\")\n",
    "print(data_rag.loc[:, 'score_1'].mean())\n",
    "print(data_rag.loc[:, 'score_2'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "93bb4458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Client.count_collections of <chromadb.api.client.Client object at 0x7462ad261610>>\n",
      "{'ids': [[]], 'embeddings': None, 'documents': [[]], 'uris': None, 'included': ['metadatas', 'documents', 'distances'], 'data': None, 'metadatas': [[]], 'distances': [[]]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nev/miniconda3/envs/prism/lib/python3.12/site-packages/bm25s/__init__.py:305: RuntimeWarning: Mean of empty slice.\n",
      "  avg_doc_len = np.array([len(doc_ids) for doc_ids in corpus_token_ids]).mean()\n",
      "/home/nev/miniconda3/envs/prism/lib/python3.12/site-packages/numpy/_core/_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "                                        \r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() iterable argument is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[160]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(client.count_collections)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(collection.query(query_texts=\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m ch_handler = \u001b[43mch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mChromaDBHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(query_summary_results_formatted)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(context_query)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Bachelor/SynKno/src/chromadb_handler.py:21\u001b[39m, in \u001b[36mChromaDBHandler.__init__\u001b[39m\u001b[34m(self, collection)\u001b[39m\n\u001b[32m     19\u001b[39m docs = \u001b[38;5;28mself\u001b[39m.get_infos_collection()[\u001b[32m0\u001b[39m]\n\u001b[32m     20\u001b[39m \u001b[38;5;28mself\u001b[39m.retriever = bm25s.BM25(corpus=docs)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbm25s\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/prism/lib/python3.12/site-packages/bm25s/__init__.py:501\u001b[39m, in \u001b[36mBM25.index\u001b[39m\u001b[34m(self, corpus, create_empty_token, show_progress, leave_progress)\u001b[39m\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m create_empty_token:\n\u001b[32m    500\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inferred_corpus_obj != \u001b[33m\"\u001b[39m\u001b[33mtoken_ids\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m vocab_dict:\n\u001b[32m--> \u001b[39m\u001b[32m501\u001b[39m         vocab_dict[\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab_dict\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m + \u001b[32m1\u001b[39m\n\u001b[32m    503\u001b[39m \u001b[38;5;28mself\u001b[39m.scores = scores\n\u001b[32m    504\u001b[39m \u001b[38;5;28mself\u001b[39m.vocab_dict = vocab_dict\n",
      "\u001b[31mValueError\u001b[39m: max() iterable argument is empty"
     ]
    }
   ],
   "source": [
    "import chromadb_handler\n",
    "importlib.reload(chromadb_handler)\n",
    "\n",
    "print(client.count_collections)\n",
    "print(collection.query(query_texts=\"test\"))\n",
    "\n",
    "ch_handler = ch.ChromaDBHandler(collection=collection)\n",
    "\n",
    "print(query_summary_results_formatted)\n",
    "print(context_query)\n",
    "res, scor = ch_handler.return_k_query_results(\"does the fish purr like a cat?\")\n",
    "print(res, scor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df8965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.15117524564266205,\n",
       "  'token': 2943,\n",
       "  'token_str': 'energy',\n",
       "  'sequence': 'plants create energy through a process known as photosynthesis.'},\n",
       " {'score': 0.1459401696920395,\n",
       "  'token': 4870,\n",
       "  'token_str': 'flowers',\n",
       "  'sequence': 'plants create flowers through a process known as photosynthesis.'},\n",
       " {'score': 0.08225198835134506,\n",
       "  'token': 9325,\n",
       "  'token_str': 'sunlight',\n",
       "  'sequence': 'plants create sunlight through a process known as photosynthesis.'},\n",
       " {'score': 0.04291250929236412,\n",
       "  'token': 18670,\n",
       "  'token_str': 'algae',\n",
       "  'sequence': 'plants create algae through a process known as photosynthesis.'},\n",
       " {'score': 0.0376516729593277,\n",
       "  'token': 7722,\n",
       "  'token_str': 'oxygen',\n",
       "  'sequence': 'plants create oxygen through a process known as photosynthesis.'}]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead89ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prism",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
