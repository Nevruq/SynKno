import requests
import json
from openai import OpenAI
from pydantic import BaseModel

MODEL_NAME_2 = "gpt-5-nano-2025-08-07"
MODEL_NAME = "gpt-3.5-turbo-0125"
#change

def call_llm_categories(user_prompt, model):
    #generate_prompt(url, "What color is the sun?")

    system_prompt = """
        You are a query rewriter for a Retrieval-Augmented Generation (RAG) system.
        Your task:
        Given a long, natural user question, you must:
        1. Extract one short and precise *core query* (max 5 words).
        2. Generate up to 3 *subqueries*, each covering a different semantic aspect of the user question (max 5 words each).
        """
    prompt = system_prompt + " This is the Prompt to Segment: " + user_prompt

    class subQueries(BaseModel):
        core_prompt: str
        subQuerie1: str
        subQuerie2: str
        subQuerie3: str

    response = model.responses.parse(model=MODEL_NAME_2, input=prompt, text_format=subQueries)
    print(response.output_text)
    return response
    # schema generated by ChatGPT
        

    # Passe maxTokens an um antwortzeit zu verkürzen

def call_llm_compare_answers(prompt, ideal_answer, answer_destilled, answer_regular, model):

    class comparisonScore(BaseModel):
        answer1_score: float
        answer2_score: float
        explanation: str


    prompt_llm = f"""
    Compare the two answers to the reference answer and score them.

    Scoring rules:
    - Score each answer from 0 to 1:
    • 1.0 = fully correct and complete  
    • 0.5 = partially correct or incomplete  
    • 0.0 = incorrect, irrelevant, or hallucinated
    - Judge based on correctness, completeness, precision, and relevance.

    Return your evaluation strictly in the structured format.
    -------------------
    QUESTION:
    {prompt}

    REFERENCE:
    {ideal_answer}

    ANSWER 1:
    {answer_destilled}

    ANSWER 2:
    {answer_regular}
    """
    response = model.responses.parse(model=MODEL_NAME_2, input=prompt_llm, text_format=comparisonScore)
    return response


def call_llm_answer_RAG(query_results, user_prompt, model):
    

    llm_prompt = f"""
    You are an assistant system that answers questions only based on the provided document excerpts.
    Rules:
    Use only the given context.
    Do not invent facts.
    CONTEXT:
    [chunk_01]
    {query_results[0]}
    [chunk_02]
    {query_results[1]}
    [chunk_03]
    {query_results[2]}
    QUESTION:
    {user_prompt}
    TASK:
    Answer the question strictly based on the context.  
    """

    result = model.responses.create(model=MODEL_NAME, input=llm_prompt)

    return result

def call_llm_regular(user_prompt, model):
    llm_prompt =  f"""
    You are a helpful and precise assistant.

    Task:
    - Answer questions only based on your trained knowledge.
    - You receive NO additional context from documents or external sources.
    Rules:
    - Answer factually and as precisely as possible.
    - If you are unsure about a fact, answer cautiously (“probably…”, “it is common that…”) or state that you are not certain.
    - If the question requires information you do not know or that is beyond your knowledge, clearly say that you cannot answer it with certainty.
    - Do not invent sources, quotes, or specific numbers if you are not sure about them.
    - If the question is ambiguous, make a reasonable assumption and briefly state what assumption you made.
    Question: {user_prompt}
    """
    result = model.responses.create(model=MODEL_NAME, input=llm_prompt)

    return result

if __name__ == "__main__":
    pass