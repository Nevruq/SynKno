import requests
import json
from openai import OpenAI
from pydantic import BaseModel

MODEL_NAME_2 = "gpt-5-nano-2025-08-07"
MODEL_NAME = "gpt-3.5-turbo-0125"
#change

def call_llm_categories(user_prompt, model):
    #generate_prompt(url, "What color is the sun?")

    system_prompt = """
        You are a query rewriter for a Retrieval-Augmented Generation (RAG) system.
        Your task:
        Given a long, natural user question, you must:
        1. Extract one short and precise *core query* (max 5 words).
        2. Generate up to 3 *subqueries*, each covering a different semantic aspect of the user question (max 5 words each).
        """
    prompt = system_prompt + " This is the Prompt to Segment: " + user_prompt

    class subQueries(BaseModel):
        core_prompt: str
        subQuerie1: str
        subQuerie2: str
        subQuerie3: str

    response = model.responses.parse(model=MODEL_NAME_2, input=prompt, text_format=subQueries)
    print(response.output_text)
    return response
    # schema generated by ChatGPT
        

    # Passe maxTokens an um antwortzeit zu verkürzen

def call_llm_compare_answers(prompt, ideal_answer, answer_destilled, answer_regular, model):

    class comparisonScore(BaseModel):
        answer1_score: float
        answer2_score: float
        explanation: str


    prompt_llm = f"""
    Compare the two answers to the reference answer and score them.

    Scoring rules:
    - Score each answer from 0 to 1:
    • 1.0 = fully correct and complete  
    • 0.5 = partially correct or incomplete  
    • 0.0 = incorrect, irrelevant, or hallucinated
    - Judge based on correctness, completeness, precision, and relevance.

    Return your evaluation strictly in the structured format.
    -------------------
    QUESTION:
    {prompt}

    REFERENCE:
    {ideal_answer}

    ANSWER 1:
    {answer_destilled}

    ANSWER 2:
    {answer_regular}
    """
    response = model.responses.parse(model=MODEL_NAME_2, input=prompt_llm, text_format=comparisonScore)
    return response


def call_llm_answer_RAG(query_results, user_prompt, model):
    
    instructions = f"""
    You are an assistant system that answers questions only based on the provided document excerpts.
    Rules:
    Use only the given context.
    Do not invent facts.
    Answer the question strictly based on the context. 
    """
    

    result = model.responses.create(model=MODEL_NAME, instructions=instructions, input=user_prompt)

    return result

def call_llm_regular(user_prompt, model):
    llm_prompt =  f"""
    You are a helpful and precise assistant.

    Task:
    - Answer questions only based on your trained knowledge.
    - You receive NO additional context from documents or external sources.
    Rules:
    - Answer factually and as precisely as possible.
    - If you are unsure about a fact, answer cautiously (“probably…”, “it is common that…”) or state that you are not certain.
    - If the question requires information you do not know or that is beyond your knowledge, clearly say that you cannot answer it with certainty.
    - Do not invent sources, quotes, or specific numbers if you are not sure about them.
    - If the question is ambiguous, make a reasonable assumption and briefly state what assumption you made.
    Question: {user_prompt}
    """
    result = model.responses.create(model=MODEL_NAME, input=llm_prompt)

    return result

def gen_QA_nasdaq(model, context):
    class qaFormatNasdaq(BaseModel):
        question: str
        answer: str

    llm_prompt = """
    You are an AI assistant that generates synthetic training data for a Retrieval-Augmented Generation (RAG) system.

    Your task:
    Using ONLY the information from the context below, create:

    1. A clear and slightly elaborate question in English that can be answered exclusively from the given context.
    2. A concise and factually correct answer in English, strictly based on the context.
    3. Do not mention that you created the question using the context.

    Rules:
    - Do NOT add information that is not present in the context.
    - Do NOT invent details, numbers, or events.
    - The question must require understanding of the context, not general world knowledge.
    - The answer must be directly supported by the context.
    - Output MUST follow this exact format:

    Give me the output in the JSON format
    {"question": txt, "answer":txt}
    CONTEXT:
    """
    formated_prompt = llm_prompt + "\n " + context
    response = model.responses.create(model=MODEL_NAME_2, input=formated_prompt)
    #test
    return response

if __name__ == "__main__":
    pass