{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bf08607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import chromadb_handler as CH\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c048fafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_dataset_4.jsonl\n",
      "2023.findings-emnlp.29.pdf\n",
      "FERTIG! Datei rag_dataset.jsonl generiert.\n",
      "rag_dataset_1.jsonl\n",
      "rag_dataset_2.jsonl\n",
      "2025.konvens-1.22.pdf\n",
      "FERTIG! Datei rag_dataset.jsonl generiert.\n",
      "rag_dataset_3.jsonl\n",
      "1-s2.0-S2352711024004047-main.pdf\n",
      "FERTIG! Datei rag_dataset.jsonl generiert.\n",
      "BAUMARTZ_Daniel_DUUI__A_Toolbox_for_the_Construction_of_a_ne.pdf\n",
      "FERTIG! Datei rag_dataset.jsonl generiert.\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# turn true if rerun of data\n",
    "cast_data = True\n",
    "\n",
    "if cast_data:\n",
    "    directory_str = \"data/DUUIDataset/training\"\n",
    "    directory = os.fsencode(directory_str)\n",
    "    counter = 1    \n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        print(filename)\n",
    "        if filename.endswith(\".pdf\") : \n",
    "            PDF_PATH = directory_str + \"/\" + filename\n",
    "            CHUNK_SIZE = 300  # Anzahl Zeichen, ≈ 500–700 Tokens\n",
    "\n",
    "            # -----------------------------\n",
    "            # 1. PDF Laden\n",
    "            # -----------------------------\n",
    "            pdf_reader = PyPDF2.PdfReader(PDF_PATH)\n",
    "\n",
    "            # Metadaten aus PDF\n",
    "            meta = pdf_reader.metadata\n",
    "\n",
    "            title = meta.title if meta and meta.title else \"Unknown Title\"\n",
    "            authors = meta.author.split(\",\") if meta and meta.author else []\n",
    "\n",
    "            # -----------------------------\n",
    "            # 2. Seiteninhalt extrahieren\n",
    "            # -----------------------------\n",
    "            pages = []\n",
    "            for i, page in enumerate(pdf_reader.pages):\n",
    "                text = page.extract_text()\n",
    "                pages.append({\"page\": i+1, \"text\": text})\n",
    "\n",
    "            # -----------------------------\n",
    "            # 3. Chunks bauen\n",
    "            # -----------------------------\n",
    "            def chunk_text(text, size=CHUNK_SIZE):\n",
    "                chunks = []\n",
    "                for i in range(0, len(text), size):\n",
    "                    chunks.append(text[i:i+size])\n",
    "                return chunks\n",
    "\n",
    "            dataset = []\n",
    "            doc_id = Path(PDF_PATH).stem\n",
    "\n",
    "            for page in pages:\n",
    "                text = page[\"text\"]\n",
    "                if not text:\n",
    "                    continue\n",
    "\n",
    "                chs = chunk_text(text)\n",
    "\n",
    "                for idx, chunk in enumerate(chs):\n",
    "                    entry = {\n",
    "                        \"id\": doc_id,\n",
    "                        \"chunk_id\": f\"{doc_id}_p{page['page']}_c{idx}\",\n",
    "                        \"source\": PDF_PATH,\n",
    "                        \"title\": title,\n",
    "                        \"authors\": authors,\n",
    "                        \"publication_year\": \"None\",\n",
    "                        \"page_start\": page[\"page\"],\n",
    "                        \"page_end\": page[\"page\"],\n",
    "                        \"text\": chunk,\n",
    "                        \"embedding\": None\n",
    "                    }\n",
    "                    dataset.append(entry)\n",
    "\n",
    "            # -----------------------------\n",
    "            # 4. JSON exportieren\n",
    "            # -----------------------------\n",
    "            with open(f\"rag_dataset_{counter}.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "                for item in dataset:\n",
    "                    f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "            counter += 1\n",
    "            print(\"FERTIG! Datei rag_dataset.jsonl generiert.\")\n",
    "\n",
    "\n",
    "\n",
    "            continue\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "405b9ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "ch = CH.chromaDBWrapper()\n",
    "client = chromadb.PersistentClient(\"chroma\")\n",
    "client.delete_collection(name=\"DUUI_300\")\n",
    "client.get_or_create_collection(name=\"DUUI_300\")\n",
    "collection = client.get_collection(\"DUUI_300\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "abd50256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/DUUIDataset/training/rag_dataset_4.jsonl\n",
      "data/DUUIDataset/training/2023.findings-emnlp.29.pdf\n",
      "data/DUUIDataset/training/rag_dataset_1.jsonl\n",
      "data/DUUIDataset/training/rag_dataset_2.jsonl\n",
      "data/DUUIDataset/training/2025.konvens-1.22.pdf\n",
      "data/DUUIDataset/training/rag_dataset_3.jsonl\n",
      "data/DUUIDataset/training/1-s2.0-S2352711024004047-main.pdf\n",
      "data/DUUIDataset/training/BAUMARTZ_Daniel_DUUI__A_Toolbox_for_the_Construction_of_a_ne.pdf\n"
     ]
    }
   ],
   "source": [
    "# Insert JSONL data into the ChromaDB\n",
    "\n",
    "import PyPDF2\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# turn true if rerun of data\n",
    "cast_data = True\n",
    "\n",
    "embedding_structure = []\n",
    "ids_fl = []  \n",
    "metadatas_fl = []\n",
    "documents_fl = []\n",
    "uris_fl = []\n",
    "if cast_data:\n",
    "    directory_str = \"data/DUUIDataset/training\"\n",
    "    directory = os.fsencode(directory_str)\n",
    "    counter = 1  \n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        str_filename = directory_str + \"/\" + filename\n",
    "        print(str_filename)\n",
    "        if filename.endswith(\".jsonl\") : \n",
    "            with open(str_filename) as f:\n",
    "                data = [json.loads(line) for line in f]\n",
    "                for line in data:\n",
    "                    #ids\n",
    "                    ids_fl.append(line[\"chunk_id\"])\n",
    "                    # metadeta\n",
    "                    if isinstance(authors, list):\n",
    "                        authors = \", \".join(authors)\n",
    "                    metadeta_dict = {\"title\": line[\"title\"], \n",
    "                                     \"authors\": authors, \n",
    "                                     \"publication_year\": line[\"publication_year\"], \n",
    "                                     \"page_start\": line[\"page_start\"], \n",
    "                                     \"page_end\": line[\"page_end\"]}\n",
    "                    metadatas_fl.append(metadeta_dict)\n",
    "                    # Docuemnts / text\n",
    "                    documents_fl.append(line[\"text\"])\n",
    "                    # Uris\n",
    "                    uris_fl.append(line[\"source\"])\n",
    "            #cast JSON arugment into proper Form\n",
    "\n",
    "\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "#add metadatas\n",
    "collection.add(ids=ids_fl, metadatas=metadatas_fl, documents=documents_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "be8cd296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>guid</th>\n",
       "      <th>link</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ukraine: Angry Zelensky vows to punish Russian...</td>\n",
       "      <td>Mon, 07 Mar 2022 08:01:56 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-europe-60638042</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-europe-606380...</td>\n",
       "      <td>The Ukrainian president says the country will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>War in Ukraine: Taking cover in a town under a...</td>\n",
       "      <td>Sun, 06 Mar 2022 22:49:58 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-europe-60641873</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-europe-606418...</td>\n",
       "      <td>Jeremy Bowen was on the frontline in Irpin, as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ukraine war 'catastrophic for global food'</td>\n",
       "      <td>Mon, 07 Mar 2022 00:14:42 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/business-60623941</td>\n",
       "      <td>https://www.bbc.co.uk/news/business-60623941?a...</td>\n",
       "      <td>One of the world's biggest fertiliser firms sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Manchester Arena bombing: Saffie Roussos's par...</td>\n",
       "      <td>Mon, 07 Mar 2022 00:05:40 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/uk-60579079</td>\n",
       "      <td>https://www.bbc.co.uk/news/uk-60579079?at_medi...</td>\n",
       "      <td>The parents of the Manchester Arena bombing's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ukraine conflict: Oil price soars to highest l...</td>\n",
       "      <td>Mon, 07 Mar 2022 08:15:53 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/business-60642786</td>\n",
       "      <td>https://www.bbc.co.uk/news/business-60642786?a...</td>\n",
       "      <td>Consumers are feeling the impact of higher ene...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Ukraine: Angry Zelensky vows to punish Russian...   \n",
       "1  War in Ukraine: Taking cover in a town under a...   \n",
       "2         Ukraine war 'catastrophic for global food'   \n",
       "3  Manchester Arena bombing: Saffie Roussos's par...   \n",
       "4  Ukraine conflict: Oil price soars to highest l...   \n",
       "\n",
       "                         pubDate  \\\n",
       "0  Mon, 07 Mar 2022 08:01:56 GMT   \n",
       "1  Sun, 06 Mar 2022 22:49:58 GMT   \n",
       "2  Mon, 07 Mar 2022 00:14:42 GMT   \n",
       "3  Mon, 07 Mar 2022 00:05:40 GMT   \n",
       "4  Mon, 07 Mar 2022 08:15:53 GMT   \n",
       "\n",
       "                                               guid  \\\n",
       "0  https://www.bbc.co.uk/news/world-europe-60638042   \n",
       "1  https://www.bbc.co.uk/news/world-europe-60641873   \n",
       "2      https://www.bbc.co.uk/news/business-60623941   \n",
       "3            https://www.bbc.co.uk/news/uk-60579079   \n",
       "4      https://www.bbc.co.uk/news/business-60642786   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.bbc.co.uk/news/world-europe-606380...   \n",
       "1  https://www.bbc.co.uk/news/world-europe-606418...   \n",
       "2  https://www.bbc.co.uk/news/business-60623941?a...   \n",
       "3  https://www.bbc.co.uk/news/uk-60579079?at_medi...   \n",
       "4  https://www.bbc.co.uk/news/business-60642786?a...   \n",
       "\n",
       "                                         description  \n",
       "0  The Ukrainian president says the country will ...  \n",
       "1  Jeremy Bowen was on the frontline in Irpin, as...  \n",
       "2  One of the world's biggest fertiliser firms sa...  \n",
       "3  The parents of the Manchester Arena bombing's ...  \n",
       "4  Consumers are feeling the impact of higher ene...  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/BBCNews/bbc_news.csv\")[:10000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8536651b",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "ValueError: Batch size of 9546 is greater than max batch size of 5461",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInternalError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[133]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m         ids_b.append(\u001b[38;5;28mid\u001b[39m)\n\u001b[32m     40\u001b[39m collection_b = client.get_or_create_collection(name=\u001b[33m\"\u001b[39m\u001b[33mbbc_news\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mcollection_b\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuemnts_b\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/prism/lib/python3.12/site-packages/chromadb/api/models/Collection.py:95\u001b[39m, in \u001b[36mCollection.add\u001b[39m\u001b[34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Add embeddings to the data store.\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[33;03m    ids: The ids of the embeddings you wish to add\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     83\u001b[39m \n\u001b[32m     84\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     86\u001b[39m add_request = \u001b[38;5;28mself\u001b[39m._validate_and_prepare_add_request(\n\u001b[32m     87\u001b[39m     ids=ids,\n\u001b[32m     88\u001b[39m     embeddings=embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m     92\u001b[39m     uris=uris,\n\u001b[32m     93\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_add\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43membeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadatas\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43muris\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muris\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/prism/lib/python3.12/site-packages/chromadb/api/rust.py:441\u001b[39m, in \u001b[36mRustBindingsAPI._add\u001b[39m\u001b[34m(self, ids, collection_id, embeddings, metadatas, documents, uris, tenant, database)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_add\u001b[39m(\n\u001b[32m    421\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    429\u001b[39m     database: \u001b[38;5;28mstr\u001b[39m = DEFAULT_DATABASE,\n\u001b[32m    430\u001b[39m ) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    431\u001b[39m     \u001b[38;5;28mself\u001b[39m.product_telemetry_client.capture(\n\u001b[32m    432\u001b[39m         CollectionAddEvent(\n\u001b[32m    433\u001b[39m             collection_uuid=\u001b[38;5;28mstr\u001b[39m(collection_id),\n\u001b[32m   (...)\u001b[39m\u001b[32m    438\u001b[39m         )\n\u001b[32m    439\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m        \u001b[49m\u001b[43muris\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mInternalError\u001b[39m: ValueError: Batch size of 9546 is greater than max batch size of 5461"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "\n",
    "df = pd.read_csv(\"data/BBCNews/bbc_news.csv\")[:5000]\n",
    "df = df.sample(n=5000, random_state=42) \n",
    "# set properties\n",
    "\n",
    "ids_b = []\n",
    "metadatas_b = []\n",
    "docuemnts_b = []\n",
    "unique_set = set()\n",
    "for row in df.itertuples(index=False):\n",
    "    # Create a short hash from full URL\n",
    "    # form year_shortURl_Hash of whole URL\n",
    "    # create unique ID\n",
    "    domain = urlparse(row.link).netloc.replace(\".\", \"_\")\n",
    "    hash_part = hashlib.md5((row.link+row.title).encode(\"utf-8\")).hexdigest()[:16]\n",
    "    date = row.pubDate\n",
    "    dt = datetime.strptime(date, \"%a, %d %b %Y %H:%M:%S %Z\")\n",
    "    id = hash_part+\"_\"+str(dt.year)\n",
    "    # text/documents\n",
    "    document = row.description\n",
    "    #metadata\n",
    "    metadeta_dict = {\"title\": row.title, \n",
    "                     \"authors\": authors, \n",
    "                                     \"publication_date\": row.pubDate, \n",
    "                                     \"link\" : row.link,\n",
    "                                     \"guide\": row.guid}\n",
    "\n",
    "    # check if id (hash of the )\n",
    "    if id not in unique_set:\n",
    "        unique_set.add(id)\n",
    "        # if not in unique set\n",
    "        metadatas_b.append(metadeta_dict)\n",
    "        docuemnts_b.append(row.description)\n",
    "        ids_b.append(id)\n",
    "    \n",
    "collection_b = client.get_or_create_collection(name=\"bbc_news\")\n",
    "collection_b.add(ids=ids_b, metadatas=metadatas_b, documents=docuemnts_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b129a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/BBCNews/bbc_news.csv\")[:10000]\n",
    "output_file = open(\"output_LLM.txt\", \"w\", encoding=\"utf-8\")\n",
    "for row in df.itertuples(index=False):\n",
    "    output_file.write(f\"{row.description} \\n\")\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8987c79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = \"Mon, 07 Mar 2022 08:01:56 GMT\"\n",
    "dt = datetime.strptime(date_str, \"%a, %d %b %Y %H:%M:%S %Z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a9cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prism",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
